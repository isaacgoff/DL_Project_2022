{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Isaac_Deep_Learning_Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWtyCG71UPaBy5HiKiuzUc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacgoff/DL_Project_2022/blob/master/Isaac_Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (Must allow access manually when prompted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LPVMtF8OkZF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models Class"
      ],
      "metadata": {
        "id": "oBMc4Ev9jCc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Jzw1G4i9bo"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.functional import softmax\n",
        "import torchvision.models as models\n",
        "\n",
        "# List of models to choose from. Currently in list:\n",
        "#   * Basic 4 layer CNN\n",
        "#   * AlexNet\n",
        "#   * VGG16\n",
        "#   * ResNet18\n",
        "class Models():\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_list = ['Basic_4_Layer_CNN', 'Alex_Net', 'VGG_16', 'Res_Net_18']\n",
        "        self.input_model = model_name\n",
        "        self.num_output_classes = 11\n",
        "        if self.input_model not in self.model_list:\n",
        "            raise ValueError('Model list does not contain model \"%s\"' %(model_name))\n",
        "    \n",
        "    def choose_model(self):\n",
        "        if self.input_model == 'Basic_4_Layer_CNN':\n",
        "            model = Basic_4_Layer_CNN()\n",
        "        elif self.input_model == 'Alex_Net':\n",
        "            model = models.alexnet(False, False)\n",
        "            model.classifier[6] = nn.Linear(in_features=4096, out_features=self.num_output_classes, bias=True)\n",
        "            model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "        elif self.input_model == 'VGG_16':\n",
        "            model = models.vgg16(False, False)\n",
        "            model.classifier[6] = nn.Linear(in_features=4096, out_features=self.num_output_classes, bias=True)\n",
        "            model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "        elif self.input_model == 'Res_Net_18':\n",
        "            model = models.resnet18(False, False)\n",
        "            model.fc = nn.Linear(in_features=512, out_features=self.num_output_classes, bias=True)\n",
        "            model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        return model\n",
        "\n",
        "\n",
        "class Basic_4_Layer_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(                                       # Dimension starts with 1 of 128 x 128\n",
        "            # larger kernel CNN layers\n",
        "            nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),       # Dimension becomes 6 of 128 x 128\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 6 of 64 x 64\n",
        "            nn.Conv2d(6, 16, kernel_size=5, padding=2), nn.ReLU(),      # Dimension now 16 of 64 x 64\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 16 of 32 x 32\n",
        "            # smaller kernel CNN layers\n",
        "            nn.Conv2d(16, 24, kernel_size=3, padding=1), nn.ReLU(),     # Dimension now 24 of 32 x 32\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 24 of 16 x 16\n",
        "            nn.Conv2d(24, 30, kernel_size=3, padding=1), nn.ReLU(),     # Dimension now 30 of 16 x 16\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 30 of 8 x 8\n",
        "            # fully connected layers\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(30 * 8 * 8, 200), nn.ReLU(),\n",
        "            nn.Linear(200, 100), nn.ReLU(),\n",
        "            nn.Linear(100, 11)                                          # Because we have 11 output classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return softmax(self.net(x), dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset Module"
      ],
      "metadata": {
        "id": "R_6GcTqEjGEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def create_dataset(audio_input_path, json_path):\n",
        "    # Load JSON file data\n",
        "    file = open(json_path, 'rb')\n",
        "    metadata = json.load(file)\n",
        "    file.close()\n",
        "    # Create list of audio files\n",
        "    sample_list = os.listdir(audio_input_path)\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    # Loop through files and store spectrogram and instrument family for each sample\n",
        "    for file in sample_list:\n",
        "        labels.append(metadata[file[:-4]]['instrument_family'])\n",
        "        # load the waveform y and sampling rate sr\n",
        "        y, sr = librosa.load(f'{audio_input_path}{file}', sr=None)\n",
        "        # convert to 2 dimensional spectogram format\n",
        "        spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000, hop_length=502)\n",
        "        # Convert raw power to dB\n",
        "        S_dB = librosa.power_to_db(spectrogram, ref=np.max)\n",
        "        data.append(S_dB)\n",
        "\n",
        "    data_np = torch.tensor(np.stack(data))\n",
        "    # labels = F.one_hot(torch.tensor(np.stack(labels)), num_classes=11)\n",
        "    labels = F.one_hot(torch.tensor(np.stack(labels)), num_classes=11).type(torch.float32)\n",
        "    return AudioSpectogramDataset(data_np, labels)\n",
        "\n",
        "\n",
        "class AudioSpectogramDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n"
      ],
      "metadata": {
        "id": "AvMI_qBLjNK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Module"
      ],
      "metadata": {
        "id": "-n5njS6yjPUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Train the individual Transformer model')\n",
        "    parser.add_argument('-f')  # Required for argument parser to work in Colab\n",
        "    parser.add_argument('--train_folder', type=str, default='debug/')\n",
        "    parser.add_argument('--val_folder', type=str, default='debug/')\n",
        "    parser.add_argument('--model', type=str, default='Res_Net_18')\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--lr', type=float, default=.01)\n",
        "    parser.add_argument('--num_epochs', type=int, default=20)\n",
        "    parser.add_argument('--status_interval', type=int, default=1)\n",
        "    parser.add_argument('--model_name', type=str, default='unspecified')\n",
        "    parser.add_argument('--save_model', type=str, default='False')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.save_model.lower() == 'true':\n",
        "        save_trained_model = True\n",
        "    else:\n",
        "        save_trained_model = False\n",
        "\n",
        "    drive_path = '/content/drive/MyDrive/DL_data/'\n",
        "    json_path_tng = f'{drive_path}nsynth-train/examples.json'\n",
        "    json_path_val = f'{drive_path}nsynth-valid/examples.json'\n",
        "    audio_input_path_tng = f'{drive_path}nsynth-train/{args.train_folder}'\n",
        "    audio_input_path_val = f'{drive_path}nsynth-valid/{args.val_folder}'\n",
        "\n",
        "    # Select GPU for runtime if available\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device(\"cpu\")\n",
        "        print('No GPU selected')\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(torch.cuda.get_device_name(device))\n",
        "\n",
        "    start = datetime.now()\n",
        "    # Create datasets\n",
        "    tng_dataset = create_dataset(audio_input_path_tng, json_path_tng)\n",
        "    val_dataset = create_dataset(audio_input_path_val, json_path_val)\n",
        "\n",
        "    # Create Data Loaders\n",
        "    tng_dataloader = DataLoader(tng_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    print(f'\\nDatasets created in {datetime.now()-start}')\n",
        "\n",
        "    # Load model\n",
        "    net = Models(args.model).choose_model().to(device)\n",
        "\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    net.apply(init_weights)\n",
        "\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    epoch = 0\n",
        "    epoch_results =[]\n",
        "    for i in range(args.num_epochs):\n",
        "        epoch_tng_loss = 0\n",
        "        epoch_tng_acc = 0\n",
        "        epoch_val_loss = 0\n",
        "        epoch_val_acc = 0\n",
        "        epoch_result = {'epoch': epoch}\n",
        "\n",
        "        # Training Loop\n",
        "        net.train()\n",
        "        n = 0\n",
        "        # print(f'\\n*** TRAINING LOOP ***\\n')\n",
        "        for (img_batch, label_batch) in tng_dataloader:\n",
        "            # print(img_batch.shape)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            img_batch = img_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "            # print(f'img_batch:\\n{img_batch}\\nlabel_batch ({label_batch.shape}):\\n{label_batch}')\n",
        "\n",
        "            img_batch = img_batch.reshape(img_batch.shape[0], 1, img_batch.shape[1], img_batch.shape[2])\n",
        "            # print(f'img_batch shape: {img_batch.shape}')\n",
        "            predicted_labels = net(img_batch)\n",
        "            # print(f'predicted_labels ({predicted_labels.shape}): {predicted_labels}')\n",
        "\n",
        "            tng_loss = loss(predicted_labels, label_batch)\n",
        "            tng_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_tng_loss += float(tng_loss.detach().item())\n",
        "            with torch.no_grad():\n",
        "                epoch_tng_acc += (predicted_labels.argmax(axis=1) == label_batch.argmax(axis=1)).sum().item()\n",
        "            n += len(label_batch)\n",
        "\n",
        "        # print(f'\\nn = {n}')\n",
        "        epoch_tng_loss /= len(tng_dataloader)\n",
        "        epoch_tng_acc /= n\n",
        "        epoch_result['tng_loss'] = epoch_tng_loss\n",
        "        epoch_result['tng_acc'] = epoch_tng_acc\n",
        "\n",
        "        # Validation Loop\n",
        "        # print(f'\\n*** VALIDATION LOOP ***\\n')\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            n = 0\n",
        "            confusion_matrix = torch.zeros(11, 11)\n",
        "            for (img_batch, label_batch) in val_dataloader:\n",
        "                img_batch = img_batch.to(device)\n",
        "                label_batch = label_batch.to(device)\n",
        "                # print(f'img_batch:\\n{img_batch}\\nlabel_batch:\\n{label_batch}')\n",
        "\n",
        "                img_batch = img_batch.reshape(img_batch.shape[0], 1, img_batch.shape[1], img_batch.shape[2])\n",
        "                predicted_labels = net(img_batch)\n",
        "                # print(f'predicted_labels: {predicted_labels}')\n",
        "\n",
        "                val_loss = loss(predicted_labels, label_batch)\n",
        "                epoch_val_loss += float(val_loss.item())\n",
        "                epoch_val_acc += (predicted_labels.argmax(axis=1) == label_batch.argmax(axis=1)).sum().item()\n",
        "                n += len(label_batch)\n",
        "\n",
        "                # calculate confusion matrix elements\n",
        "                for j in range(len(label_batch)):\n",
        "                    confusion_matrix[torch.argmax(label_batch[j, :])][torch.argmax(predicted_labels[j, :])] += 1\n",
        "\n",
        "            # print(f'\\nn = {n}')\n",
        "            epoch_val_loss /= len(val_dataloader)\n",
        "            epoch_val_acc /= n\n",
        "            epoch_result['val_loss'] = epoch_val_loss\n",
        "            epoch_result['val_acc'] = epoch_val_acc\n",
        "            label_counts = torch.sum(confusion_matrix, dim=1).reshape(len(confusion_matrix), 1)\n",
        "            confusion_matrix /= label_counts\n",
        "\n",
        "        epoch_results.append(epoch_result)\n",
        "        if epoch % args.status_interval == 0:\n",
        "            print(f'epoch {epoch} completed: Training Loss = {epoch_tng_loss} //'\n",
        "                  f' Training Score = {epoch_tng_acc} // Validation Score = {epoch_val_acc}')\n",
        "\n",
        "        # Establish training cutoff criteria\n",
        "        if epoch == 0:\n",
        "            max_val_acc = epoch_val_acc\n",
        "            best_model_state = deepcopy(net.state_dict())\n",
        "            best_confusion_matrix = confusion_matrix\n",
        "        elif epoch_val_acc > max_val_acc:\n",
        "            # print(f'new minimum loss achieved at epoch {epoch}', file=output_file)\n",
        "            max_val_acc = epoch_val_acc\n",
        "            best_model_state = deepcopy(net.state_dict())  # Save state of model with minimum validation loss\n",
        "            best_confusion_matrix = confusion_matrix\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    # Call function to generate performance data\n",
        "    plot_model_results(epoch_results)\n",
        "\n",
        "    # Display confusion matrix\n",
        "    print(f'Confusion Matrix:\\n {best_confusion_matrix}')\n",
        "    plot_confusion_matrix(best_confusion_matrix)\n",
        "\n",
        "    # Save the best model state for future use\n",
        "    if save_trained_model:\n",
        "        torch.save(best_model_state, f'{drive_path}nsynth-models/{args.model_name}')\n",
        "\n",
        "    end = datetime.now()\n",
        "    print(f'\\nelapsed time: {end - start}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "S9VHBmljjO0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Plots Module"
      ],
      "metadata": {
        "id": "zvY8IeOKySbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_model_results(epoch_results):\n",
        "\n",
        "    # Plot training and validation loss by epoch\n",
        "    # Create lists for plotting\n",
        "    epochs, tng_losses, val_losses, tng_acc, val_acc = [], [], [], [], []\n",
        "    for epoch in epoch_results:\n",
        "        epochs.append(epoch[\"epoch\"])\n",
        "        tng_losses.append(epoch[\"tng_loss\"])\n",
        "        val_losses.append(epoch[\"val_loss\"])\n",
        "        tng_acc.append(epoch[\"tng_acc\"])\n",
        "        val_acc.append(epoch[\"val_acc\"])\n",
        "\n",
        "    # Code to plot loss values by epoch\n",
        "    plt.plot(epochs, tng_losses, label=f'Training Loss')\n",
        "    plt.plot(epochs, val_losses, label=f'Validation Loss')\n",
        "    plt.plot(epochs, tng_acc, label=f'Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, label=f'Validation Accuracy')\n",
        "    plt.title(f'Model Results by Epoch')\n",
        "    plt.xlabel(f'Epoch')\n",
        "    plt.ylabel(f'Loss and Accuracy')\n",
        "    plt.legend()\n",
        "    plt.axis([0, len(epochs), 0, 3])\n",
        "    plt.show()\n",
        "    # plt.savefig(f'/content/drive/MyDrive/DL_data/plot-results.png', dpi=150, bbox_inches='tight', facecolor='gray')\n",
        "    # plt.clf()\n"
      ],
      "metadata": {
        "id": "1poiPpKLyWwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Module"
      ],
      "metadata": {
        "id": "b0cMhGfuhJM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    cm = cm[[0, 1, 2, 3, 4, 5, 6, 7, 8, 10], :]\n",
        "    cm = cm[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 10]]\n",
        "    print('Confusion matrix')\n",
        "    classes = ('bass', 'brass', 'flute', 'guitar', 'keyboard', 'mallet', 'organ', 'reed', 'string', 'vocal')\n",
        "    plt.figure(2, figsize=(11,11))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = 0.5\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, \"{:.3f}\".format(cm[i, j]), horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "tx67a-_AhIp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Module"
      ],
      "metadata": {
        "id": "Q7Jcp5JDgf5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "# from create_dataset import create_dataset\n",
        "# from Models import Models\n",
        "\n",
        "# from Confusion_matrix_graphic import plot_confusion_matrix\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Test the individual instrument identification model')\n",
        "    parser.add_argument('-f')  # Required for argument parser to work in Colab\n",
        "    parser.add_argument('--test_folder', type=str, default='small_audio/')\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--model_name', type=str,  default='res_net')\n",
        "    parser.add_argument('--model_type', type=str, default='Res_Net_18')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Dataset and model paths\n",
        "    drive_path = '/content/drive/MyDrive/DL_data/'\n",
        "    # json_path_test = f'{drive_path}nsynth-test/examples.json'\n",
        "    # audio_input_path_test = f'{drive_path}nsynth-test/{args.test_folder}'\n",
        "    json_path_test = f'{drive_path}nsynth-valid/examples.json'\n",
        "    audio_input_path_test = f'{drive_path}nsynth-valid/{args.test_folder}'\n",
        "    model_path = f'{drive_path}nsynth-models/{args.model_name}'\n",
        "\n",
        "    # Select GPU for runtime if available\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device(\"cpu\")\n",
        "        print('No GPU selected')\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(torch.cuda.get_device_name(device))\n",
        "\n",
        "    # Add code to store and deal with output files\n",
        "\n",
        "    start = datetime.now()\n",
        "\n",
        "    # Create dataset\n",
        "    test_dataset = create_dataset(audio_input_path_test, json_path_test)\n",
        "\n",
        "    # Create Data Loader\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    print(f'\\nDatasets created in {datetime.now()-start}')\n",
        "\n",
        "    # Create instance of model and load saved weights\n",
        "    model = Models(args.model_type).choose_model().to(device)\n",
        "    model.load_state_dict(torch.load(f'{model_path}', map_location=device))\n",
        "\n",
        "    # Inference loop\n",
        "    print(\"Beginning inference loop\\n.....\")\n",
        "    test_score = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        n = 0\n",
        "\n",
        "        confusion_matrix = torch.zeros(11,11)\n",
        "\n",
        "        for (img_batch, label_batch) in test_dataloader:\n",
        "            img_batch = img_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "            # print(f'img_batch:\\n{img_batch}\\nlabel_batch:\\n{label_batch}')\n",
        "\n",
        "            img_batch = img_batch.reshape(img_batch.shape[0], 1, img_batch.shape[1], img_batch.shape[2])\n",
        "            predicted_labels = model(img_batch)\n",
        "            # print(f'predicted_labels: {predicted_labels}')\n",
        "\n",
        "            test_score += (predicted_labels.argmax(axis=1) == label_batch.argmax(axis=1)).sum().item()\n",
        "            print(f'Correct predictions in batch: {test_score}\\n')\n",
        "            \n",
        "            # calculate confusion matrix elements\n",
        "            for i in range(len(label_batch)):\n",
        "              confusion_matrix[torch.argmax(label_batch[i, :])][torch.argmax(predicted_labels[i, :])] += 1\n",
        "\n",
        "            n += len(label_batch)\n",
        "        \n",
        "        # print(f'\\nn = {n}')\n",
        "        label_counts = torch.sum(confusion_matrix, dim=1).reshape(len(confusion_matrix), 1)\n",
        "        confusion_matrix /= label_counts\n",
        "        print(f'Confusion Matrix:\\n {confusion_matrix}')\n",
        "        #Use %run not !python3 to get cm to display in collab\n",
        "        plot_confusion_matrix(confusion_matrix)\n",
        "\n",
        "        test_score = test_score / n\n",
        "        print(f'Final test accuracy: {test_score}')\n",
        "\n",
        "    end = datetime.now()\n",
        "    print(f'\\nelapsed time: {end - start}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "bBD67P9Lge7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}