{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Isaac_Deep_Learning_Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORsTKtReUIAkbpHGPdWNxW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacgoff/DL_Project_2022/blob/master/Isaac_Deep_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (Must allow access manually when prompted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPVMtF8OkZF-",
        "outputId": "b5ecdce4-f5ba-48d6-cb1d-2f551de62d4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models Class"
      ],
      "metadata": {
        "id": "oBMc4Ev9jCc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L8Jzw1G4i9bo"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.functional import softmax\n",
        "import torchvision.models as models\n",
        "\n",
        "# List of models to choose from. Currently in list:\n",
        "#   * Basic 4 layer CNN\n",
        "#   * AlexNet\n",
        "#   * VGG16\n",
        "#   * ResNet18\n",
        "class Models():\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_list = ['Basic_4_Layer_CNN', 'Alex_Net', 'VGG_16', 'Res_Net_18']\n",
        "        self.input_model = model_name\n",
        "        self.num_output_classes = 11\n",
        "        if self.input_model not in self.model_list:\n",
        "            raise ValueError('Model list does not contain model \"%s\"' %(model_name))\n",
        "    \n",
        "    def choose_model(self):\n",
        "        if self.input_model == 'Basic_4_Layer_CNN':\n",
        "            model = Basic_4_Layer_CNN()\n",
        "        elif self.input_model == 'Alex_Net':\n",
        "            model = models.alexnet(False, False)\n",
        "            model.classifier[6] = nn.Linear(in_features=4096, out_features=self.num_output_classes, bias=True)\n",
        "            model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "        elif self.input_model == 'VGG_16':\n",
        "            model = models.vgg16(False, False)\n",
        "            model.classifier[6] = nn.Linear(in_features=4096, out_features=self.num_output_classes, bias=True)\n",
        "            model.features[0] = nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
        "        elif self.input_model == 'Res_Net_18':\n",
        "            model = models.resnet18(False, False)\n",
        "            model.fc = nn.Linear(in_features=512, out_features=self.num_output_classes, bias=True)\n",
        "            model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        return model\n",
        "\n",
        "\n",
        "class Basic_4_Layer_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(                                       # Dimension starts with 1 of 128 x 128\n",
        "            # larger kernel CNN layers\n",
        "            nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.ReLU(),       # Dimension becomes 6 of 128 x 128\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 6 of 64 x 64\n",
        "            nn.Conv2d(6, 16, kernel_size=5, padding=2), nn.ReLU(),      # Dimension now 16 of 64 x 64\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 16 of 32 x 32\n",
        "            # smaller kernel CNN layers\n",
        "            nn.Conv2d(16, 24, kernel_size=3, padding=1), nn.ReLU(),     # Dimension now 24 of 32 x 32\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 24 of 16 x 16\n",
        "            nn.Conv2d(24, 30, kernel_size=3, padding=1), nn.ReLU(),     # Dimension now 30 of 16 x 16\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),                      # Dimension now 30 of 8 x 8\n",
        "            # fully connected layers\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(30 * 8 * 8, 200), nn.ReLU(),\n",
        "            nn.Linear(200, 100), nn.ReLU(),\n",
        "            nn.Linear(100, 11)                                          # Because we have 11 output classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return softmax(self.net(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset Module"
      ],
      "metadata": {
        "id": "R_6GcTqEjGEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def create_dataset(audio_input_path, json_path):\n",
        "    # Load JSON file data\n",
        "    file = open(json_path, 'rb')\n",
        "    metadata = json.load(file)\n",
        "    file.close()\n",
        "    # Create list of audio files\n",
        "    sample_list = os.listdir(audio_input_path)\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    # Loop through files and store spectrogram and instrument family for each sample\n",
        "    for file in sample_list:\n",
        "        labels.append(metadata[file[:-4]]['instrument_family'])\n",
        "        # load the waveform y and sampling rate sr\n",
        "        y, sr = librosa.load(f'{audio_input_path}{file}', sr=None)\n",
        "        # convert to 2 dimensional spectogram format\n",
        "        spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000, hop_length=502)\n",
        "        # Convert raw power to dB\n",
        "        S_dB = librosa.power_to_db(spectrogram, ref=np.max)\n",
        "        data.append(S_dB)\n",
        "\n",
        "    data_np = torch.tensor(np.stack(data))\n",
        "    # labels = F.one_hot(torch.tensor(np.stack(labels)), num_classes=11)\n",
        "    labels = F.one_hot(torch.tensor(np.stack(labels)), num_classes=11).type(torch.float32)\n",
        "    return AudioSpectogramDataset(data_np, labels)\n",
        "\n",
        "\n",
        "class AudioSpectogramDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n"
      ],
      "metadata": {
        "id": "AvMI_qBLjNK-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Module"
      ],
      "metadata": {
        "id": "-n5njS6yjPUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "from torch import nn\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "# from create_dataset import create_dataset\n",
        "# from Models import Models\n",
        "# from BasicCNN import BasicCNN\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Train the individual Transformer model')\n",
        "    parser.add_argument('-f')  # Required for argument parser to work in Colab\n",
        "    parser.add_argument('--train_folder', type=str, default='small_audio/')\n",
        "    parser.add_argument('--val_folder', type=str, default='small_audio/')\n",
        "    parser.add_argument('--model', type=str, default='Basic_4_Layer_CNN')\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--lr', type=float, default=.01)\n",
        "    parser.add_argument('--num_epochs', type=int, default=20)\n",
        "    parser.add_argument('--status_interval', type=int, default=1)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    drive_path = '/content/drive/MyDrive/DL_data/'\n",
        "    json_path_tng = f'{drive_path}nsynth-train/examples.json'\n",
        "    json_path_val = f'{drive_path}nsynth-valid/examples.json'\n",
        "    audio_input_path_tng = f'{drive_path}nsynth-train/{args.train_folder}'\n",
        "    audio_input_path_val = f'{drive_path}nsynth-valid/{args.val_folder}'\n",
        "\n",
        "    # Select GPU for runtime if available\n",
        "    if not torch.cuda.is_available():\n",
        "        device = torch.device(\"cpu\")\n",
        "        print('No GPU selected')\n",
        "    else:\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(torch.cuda.get_device_name(device))\n",
        "\n",
        "    start = datetime.now()\n",
        "    # Create datasets\n",
        "    tng_dataset = create_dataset(audio_input_path_tng, json_path_tng)\n",
        "    val_dataset = create_dataset(audio_input_path_val, json_path_val)\n",
        "\n",
        "    # Create Data Loaders\n",
        "    tng_dataloader = DataLoader(tng_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    print(f'\\nDatasets created in {datetime.now()-start}')\n",
        "\n",
        "    # Load model\n",
        "    net = Models('Basic_4_Layer_CNN').choose_model().to(device)\n",
        "\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    net.apply(init_weights)\n",
        "\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    epoch = 0\n",
        "    for i in range(args.num_epochs):\n",
        "        epoch_tng_loss = 0\n",
        "        epoch_tng_score = 0\n",
        "        epoch_val_score = 0\n",
        "\n",
        "        # Training Loop\n",
        "        net.train()\n",
        "        n = 0\n",
        "        # print(f'\\n*** TRAINING LOOP ***\\n')\n",
        "        for (img_batch, label_batch) in tng_dataloader:\n",
        "            # print(img_batch.shape)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            img_batch = img_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "            # print(f'img_batch:\\n{img_batch}\\nlabel_batch ({label_batch.shape}):\\n{label_batch}')\n",
        "\n",
        "            img_batch = img_batch.reshape(img_batch.shape[0], 1, img_batch.shape[1], img_batch.shape[2])\n",
        "            # print(f'img_batch shape: {img_batch.shape}')\n",
        "            predicted_labels = net(img_batch)\n",
        "            # print(f'predicted_labels ({predicted_labels.shape}): {predicted_labels}')\n",
        "\n",
        "            tng_loss = loss(predicted_labels, label_batch)\n",
        "            tng_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_tng_loss += float(tng_loss.detach().item())\n",
        "            with torch.no_grad():\n",
        "                epoch_tng_score += (predicted_labels.argmax(axis=1) == label_batch.argmax(axis=1)).sum().item()\n",
        "            n += len(label_batch)\n",
        "\n",
        "        # print(f'\\nn = {n}')\n",
        "        epoch_tng_loss /= len(tng_dataloader)\n",
        "        epoch_tng_score /= n\n",
        "\n",
        "        # Validation Loop\n",
        "        # print(f'\\n*** VALIDATION LOOP ***\\n')\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            n = 0\n",
        "            for (img_batch, label_batch) in val_dataloader:\n",
        "                img_batch = img_batch.to(device)\n",
        "                label_batch = label_batch.to(device)\n",
        "                # print(f'img_batch:\\n{img_batch}\\nlabel_batch:\\n{label_batch}')\n",
        "\n",
        "                img_batch = img_batch.reshape(img_batch.shape[0], 1, img_batch.shape[1], img_batch.shape[2])\n",
        "                predicted_labels = net(img_batch)\n",
        "                # print(f'predicted_labels: {predicted_labels}')\n",
        "\n",
        "                epoch_val_score += (predicted_labels.argmax(axis=1) == label_batch.argmax(axis=1)).sum().item()\n",
        "                n += len(label_batch)\n",
        "\n",
        "            # print(f'\\nn = {n}')\n",
        "            epoch_val_score /= n\n",
        "\n",
        "        if epoch % args.status_interval == 0:\n",
        "            print(f'\\nepoch {epoch} completed: Training Loss = {epoch_tng_loss} //'\n",
        "                  f' Training Score = {epoch_tng_score} // Validation Score = {epoch_val_score}')\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    end = datetime.now()\n",
        "    print(f'\\nelapsed time: {end - start}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9VHBmljjO0q",
        "outputId": "b230b79e-2bd3-47c1-8aad-0cefcf7fbfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla V100-SXM2-16GB\n"
          ]
        }
      ]
    }
  ]
}